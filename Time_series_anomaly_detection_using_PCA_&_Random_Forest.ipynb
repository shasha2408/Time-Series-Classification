{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Time series anomaly detection using PCA & Random Forest.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMIDDYW2ev0V6H12PYErDpp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shasha2408/Time-Series-Classification/blob/main/Time_series_anomaly_detection_using_PCA_%26_Random_Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs0nRm-NW6MS"
      },
      "source": [
        "#### **Data Info**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsNHcB_2WUsx"
      },
      "source": [
        "def dataset_stats(train,test):\n",
        " \n",
        " # information of train and test set\n",
        "\n",
        "  train.info()\n",
        "  test.info()\n",
        "  return \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI8XMzlsW8gi"
      },
      "source": [
        "#### **Train_test_split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPL3FBezWsVa"
      },
      "source": [
        "def train_test_split(train,test):\n",
        "\n",
        "  # spliting the train and test data in inputs and outputs. \n",
        "\n",
        "    x_train = train.iloc[:,1:]\n",
        "    y_train = train.iloc[:,0]\n",
        "    x_test = test.iloc[:,1:]\n",
        "    y_test = test.iloc[:,0]\n",
        "    print(\"x_train Shape :: \", x_train.shape)\n",
        "    print(\"y_train Shape :: \", y_train.shape)\n",
        "    print(\"x_test Shape :: \", x_test.shape)\n",
        "    print(\"y_test Shape :: \", y_test.shape)\n",
        "   \n",
        "    return  x_train,x_test,y_train,y_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TBz30NMW_Eg"
      },
      "source": [
        "#### __Data Scaling__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE0_Rh0MWsYJ"
      },
      "source": [
        "def data_scaling(x_train,x_test):\n",
        "  scaler = StandardScaler()\n",
        "  x_train_scaled = scaler.fit_transform(x_train)\n",
        "  x_test_sclaed  = scaler.fit_transform(x_test)\n",
        "  return x_train_scaled,x_test_sclaed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXY7HnAOYMHq"
      },
      "source": [
        "#### __Calculating Eigenvectors and eigenvalues of Cov matirx__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lETVd_PaYLOD"
      },
      "source": [
        "def eigenvec_eigenval(data):\n",
        "  # caslculating mean vector of training datatset\n",
        "\n",
        "  mean_vec = np.mean(data, axis=0)\n",
        "\n",
        "  # covariance matrix\n",
        "\n",
        "  cov_mat = np.cov(data.T)\n",
        "\n",
        "  eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
        "\n",
        "  # Create a list of (eigenvalue, eigenvector) tuples\n",
        "  eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
        "\n",
        "  # Sort the eigenvalue, eigenvector pair from high to low\n",
        "\n",
        "  eig_pairs.sort(key = lambda x: x[0], reverse= True)\n",
        "\n",
        "  # Calculation of Explained Variance from the eigenvalues\n",
        "\n",
        "  tot = sum(eig_vals)\n",
        "\n",
        "  # Individual explained variance\n",
        "\n",
        "  var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
        "\n",
        "  cum_var_exp = np.cumsum(var_exp)  # Cumulative explained variance \n",
        "\n",
        "  # Find the eigenvector beyond which 95% of the data is explained\n",
        "\n",
        "  \n",
        "  \n",
        "  return cum_var_exp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV7gMG5HYQsy"
      },
      "source": [
        "### __data_variance__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRVVEC-TYW-K"
      },
      "source": [
        "def plot_cum_var_exp(cum_var_exp):\n",
        "  sns.set(style='whitegrid')\n",
        "  plt.plot(cum_var_exp)\n",
        "  plt.xlabel('number of components')\n",
        "  plt.ylabel('cumulative explained variance')\n",
        "  display(plt.show())\n",
        "  return"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6mq9UXeYaeT"
      },
      "source": [
        "#### __Calculate the Principal components(p) of the originat data__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7mfrkq8YmFK"
      },
      "source": [
        "def Prin_components(cum_var_exp):\n",
        "  p = [ n for n,i in enumerate(cum_var_exp) if i>95 ][0]\n",
        "  print('no.of principal components', p)\n",
        "  return p"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxm9yA_zYp70"
      },
      "source": [
        "#### __Reducing the train and test dimension to Dimension P__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3h4FmDmYwbj"
      },
      "source": [
        "def pca_data(x_train_scaled,x_test_scaled,p):\n",
        "  from sklearn.decomposition import PCA\n",
        "  pca = PCA(n_components = p)\n",
        "  pca.fit(x_train_scaled)\n",
        "  x_train_pca= pca.transform(x_train_scaled)\n",
        "  pca.fit(x_test_scaled)\n",
        "  x_test_pca = pca.transform(x_test_scaled)\n",
        "  print(x_train_pca.shape)\n",
        "  print(x_test_pca.shape)\n",
        "  return x_train_pca,x_test_pca,p"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGcHN44IY21D"
      },
      "source": [
        "#### __RF model Building__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLLrK_t1ZBQq"
      },
      "source": [
        "def RandF_clf(x_train_pca,y_train):\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "  Rf = RandomForestClassifier(random_state=1)\n",
        "  Rf.fit(x_train_pca,y_train)\n",
        "  return Rf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpvhDeGnZPCK"
      },
      "source": [
        "#### __Evaluation__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk1LFxWpXVLC"
      },
      "source": [
        "def evaluation(y_test,predictions):\n",
        "\n",
        "  predictions = trained_model.predict(x_test_pca)\n",
        "  acc_test = accuracy_score(y_test, predictions)\n",
        "  print('Test Accuracy: %.2f' % acc_test)\n",
        "  print(classification_report(y_test, predictions)),\n",
        "  cm=metrics.confusion_matrix(y_test,predictions)\n",
        "  ax= plt.figure(figsize=(8,4))\n",
        "  ax= plt.subplot()\n",
        "  ax.set_xlabel('Predicted class');ax.set_ylabel('True class'); \n",
        "  ax.set_title('Confusion Matrix for SVM'); \n",
        "  sns.heatmap(cm, annot=True, fmt='g', ax=ax); \n",
        "  ax.xaxis.set_ticklabels(['Normal', 'Abnormal']); ax.yaxis.set_ticklabels(['Normal', 'Abnormal']);\n",
        "  plt.show()\n",
        "  return"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkQJ32ugZe1U"
      },
      "source": [
        "### __Crossvalidation and Finding the Best Model using GridSearch__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpKLX3_fZOf7"
      },
      "source": [
        "def GRid_search(x_train_pca,y_train):\n",
        "  from sklearn.model_selection import GridSearchCV\n",
        "  from sklearn.model_selection import cross_validate\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "  Rfc = RandomForestClassifier()\n",
        "  scores = cross_validate(Rfc, x_train_pca,y_train, cv=5, scoring=['accuracy'], return_train_score=True)\n",
        "  param_grid = { 'n_estimators': [200, 500],'max_depth' : [6,7,8,9,10]}\n",
        "  CV_rfc = GridSearchCV(Rfc, param_grid=param_grid, cv= 5)\n",
        "  final_model = CV_rfc.fit(x_train_pca, y_train)\n",
        "  CV_rfc.best_params_\n",
        "  CV_rfc.best_score_\n",
        "  print('Train accuracy: ', scores['train_accuracy'])\n",
        "  print('Test accuracy: ', scores['test_accuracy'])\n",
        "  return final_model"
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}